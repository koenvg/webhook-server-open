#!/usr/bin/env node

/**
 * Functions for migrating from housing individual /webhook-uploads/
 * with their respective site buckets. This is born out of the deploys
 * feature that allows for pushing different sets of templates to be
 * connected to a single CMS, and multiple site buckets. Buckets can
 * be spawned a different points, so there is a centralized 
 * /webhook-uploads/ in the defined `uploadsBucket`.
 *
 * CopyFiles issues rewrite requests against CloudStroge for each
 * object in the individual site /webhook-uploads/, and writes them
 * to the `uploadsBucket` /webhook-uploads/
 *
 * Prefix data looks at the firebase representation of each site,
 * and adds the `uploadsBucket` prefix to all `/webhook-uploads/`
 * URLs.
 */

require( 'dotenv' ).config()

var async = require( 'async' )
var traverse = require( 'traverse' )
var cheerio = require( 'cheerio' )
var cloudStorage = require( '../libs/cloudStorage.js' )
var Firebase = require( 'firebase' )

var uploadsBucket = process.env.UPLOADS_BUCKET;
var firebaseName = process.env.FIREBASE;
var firebaseKey = process.env.FIREBASE_KEY;


CopyFiles( function ( error, results ) {
  if ( error ) console.log( error )
  console.log( [ 'copied-files' results.length ].join( ':' ) )
} )

PrefixData( function ( error, results ) {
  if ( error ) console.log( error )
  console.log( JSON.stringify( results ) );
} )

 
/**
 * Copy files
 * List all buckets, then their webhook-uploads
 * List all files in the webhook-uploads of the uploadsBucket
 * Every file that exists in a site bucket & not in the uploadsBucket
 * gets copied to the uploadsBucket.
 * 
 * @param {Function} copyFilesDone Callback invoked with results of the copy
 */
function CopyFiles ( copyFilesDone ) {

  var copyToDestination = {
    destinationBucket: uploadsBucket,
  }

  cloudStorage.buckets.list( function ( error, buckets ) {

    // build up copyTaskArguments

    // copyTaskArguments = { sourceBucket }
    // do not include the uploads bucket as a source to copy from.
    var copyTaskArguments = buckets.items
      .filter( function ( bucket ) { return bucket.name !== copyToDestination.destinationBucket } )
      .map( function ( bucket ) { return { sourceBucket: bucket.name } } )

    var copyTaskArgumentsAddSourceFilesTasks = copyTaskArguments.map( addSourceFilesTasks )

    async.parallel( copyTaskArgumentsAddSourceFilesTasks, copyTaskArgumentsWithSourceKeys )

    function copyTaskArgumentsWithSourceKeys ( error, copyTaskArgumentsNestedByBucket ) {
      if ( error ) return copyFilesDone( error )

      // copyTaskArguments = { sourceBucket, sourceFile, sourceMd5Hash }
      copyTaskArguments = copyTaskArgumentsNestedByBucket.reduce( function ( previous, current ) { return previous.concat( current ) }, [] )

      // filter copyTaskArguments, removing tasks that represent destination files that already exist
      filterCopyTasksForDestination( copyTaskArguments, function ( error, remainingCopyTasks ) {

        var copyTasks = remainingCopyTasks.map( addDestionationArguments ).map( makeCopyTasks )

        async.parallelLimit( copyTasks, 20, function ( error, results ) {
          copyFilesDone( error, results )
        } )

      } )
    }

  } ) 

  /**
   * Updates copyTaskArguments with sourceFile & sourceMd5Hash values
   * sourceFile    is the file path to the source file to copy
   * sourceMd5Hash is the md5 hash of the source file to copy
   *
   * input:  copyTaskArgument = { sourceBucket }
   * output: copyTaskArgument = { sourceBucket, sourceFile, sourceMd5Hash }
   *
   * This function tasks a list of arguments, and returns async tasks
   * that will update each object with the new values.
   * 
   * @param {object} copyTaskArgument
   * @param {string} copyTaskArgument.sourceBucket
   */
  function addSourceFilesTasks ( copyTaskArgument ) {
    return function getSourceFilesTask ( taskComplete ) {

      var copyTaskArgumentsWithSourceFiles = [];

      function getUploads ( options ) {
        if ( !options ) options = {};

        cloudStorage.objects.listUploads( copyTaskArgument.sourceBucket, options, function ( error, sourceFiles ) {
          
          if ( error ) return taskComplete( error )
          if ( !sourceFiles.items ) return taskComplete( null, [] )
          
          copyTaskArgumentsWithSourceFiles = copyTaskArgumentsWithSourceFiles.concat( sourceFiles.items.map( function ( sourceFile ) {
            return Object.assign( {}, copyTaskArgument, {
                sourceFile: sourceFile.name,
                sourceMd5Hash: sourceFile.md5Hash
              } ) } ) )

          if ( sourceFiles.nextPageToken ) return getUploads( { pageToken: sourceFiles.nextPageToken } )

          taskComplete( null, copyTaskArgumentsWithSourceFiles )
        } )
      }

      getUploads()
    }
  }

  /**
   * Updates copyTaskArguments with destinationBucket & destinationFile values
   * destinationBucket is the bucket the file will be copied to
   * destinationFile   is the file path the file will be copied to
   *  
   * input:  copyTaskArgument = { sourceBucket, sourceFile, sourceMd5Hash }
   * output: copyTaskArgument = { sourceBucket, sourceFile, sourceMd5Hash, destinationBucket, destinationFile }
   * 
   * This function tasks a list of arguments, and returns async tasks
   * that will update each object with the new values.
   * 
   * @param {object} copyTaskArgument
   * @param {string} copyTaskArgument.sourceBucket
   * @param {string} copyTaskArgument.sourceFile
   * @param {string} copyTaskArgument.sourceMd5Hash
   */
  function addDestionationArguments ( copyTaskArgument ) {
    return Object.assign( copyTaskArgument, copyToDestination, { destinationFile: copyTaskArgument.sourceFile } )
  }

  /**
   * Given a list of copyTaskArguments, callback with the set of arguments
   * that represent files that do not currently exist in the destination bucket
   * with the same file name and md5 hash as the source. This means the file
   * has already been copied.
   *
   * input:  copyTaskArguments [{ sourceBucket, sourceFile, sourceMd5Hash }]
   * output: filteredCopyTaskArguemnts [{ sourceBucket, sourceFile, sourceMd5Hash }]
   *
   * @param {object}   copyTaskArguments[]
   * @param {Function} callback
   */
  function filterCopyTasksForDestination ( copyTaskArguments, callback ) {

    var allDestinationFiles = []

    function getDestinationFiles ( options ) {
      if ( !options ) options = {}

      cloudStorage.objects.listUploads( copyToDestination.destinationBucket, options, function ( error, destinationFiles ) {
        // error, return callback with error
        if ( error ) return callback( error )

        // no files in destination bucket to filter against, callback with all copy task arguments
        if ( !destinationFiles.items ) return callback( null, copyTaskArguments )

        allDestinationFiles = allDestinationFiles.concat( destinationFiles.items )

        if ( destinationFiles.nextPageToken ) return getDestinationFiles( { pageToken: destinationFiles.nextPageToken } )

        // filter copy tasks based on files in the destination bucket
        var filteredCopyTasks = copyTaskArguments.filter( argumentsForFilesThatExistInDestination )

        callback( null, filteredCopyTasks )

        function argumentsForFilesThatExistInDestination ( copyTaskArgument ) {
          // keep arguments for tasks that speak to files that do not exist
          var doesNotExist = true;
          for (var i = allDestinationFiles.length - 1; i >= 0; i--) {
            if ( allDestinationFiles[ i ].name === copyTaskArgument.sourceFile ) {
              // the source file exists in the destination bucket
              if ( allDestinationFiles[ i ].md5Hash === copyTaskArgument.sourceMd5Hash ) {
                // the source file with the same md5 has exists in the destination bucket
                doesNotExist = false;
              }
              break;
            }
          }
          return doesNotExist;
        } 

      } )
    }

    getDestinationFiles()
    
  }

  function makeCopyTasks ( copyTaskArgument ) {
    return function ( taskComplete ) {
      cloudStorage.objects.rewrite( copyTaskArgument.sourceBucket, encodeURIComponent( copyTaskArgument.sourceFile ), copyTaskArgument.destinationBucket, encodeURIComponent( copyTaskArgument.destinationFile ), function ( error, results ) {
        if ( error ) return taskComplete( error )

        taskComplete( null, results )
      } )
    }
  }

}

/**
 * Look at all values in the firebase, and prefix the migrated URLs
 * @param {object}   options
 * @param {string}   options.migratingFrom The prefix that is being migrated from. Non empty string.
 * @param {Function} onComplete
 */
function PrefixData ( options, onComplete ) {
  if ( typeof options === 'function' ) onComplete = options
  if ( !options ) options = {};
  if ( !onComplete ) onComplete === function noop () {}

  var migratingFrom = options.migratingFrom || false;

  var buckets = new Firebase( 'https://' + firebaseName + '.firebaseio.com/buckets' )

  getWebhookBucketsData( firebaseKey, function ( error, bucketRoot ) {

    var setSiteDataTasks = [];
    
    Object.keys( bucketRoot ).forEach( function ( site ) {
      Object.keys( bucketRoot[ site ] ).forEach( function ( key ) {

        traverse( bucketRoot[ site ][ key ].dev.data )
          .forEach( function ( dataValue ) {
            if ( this.isLeaf ) {
              if ( isWYSIWYG( dataValue ) ) {
                this.update( updateHTML( dataValue ) )
              }
              else if ( isUrlKey( this.path ) ) {
                this.update( updateUrl( dataValue ) )
              }
            }
          } )


        setSiteDataTasks = setSiteDataTasks.concat( [ makeSetDataTask( {
            site: site,
            key: key,
            data: Object.assign( {}, bucketRoot[ site ][ key ].dev.data )
          } ) ] )

      } )
    } )

    async.parallelLimit( setSiteDataTasks, 5, function ( error, results ) {
      onComplete( error, results )
    } )

  } )


  function getWebhookBucketsData ( secret, callback ) {
    buckets.auth( firebaseKey, function ( error ) {
      if ( error ) return callback( error );

      buckets.once( 'value',
        function ( snapshot ) {
          var bucketRoot = snapshot.val();
          callback( null, bucketRoot )
        },
        function ( error ) {
          callback( error )
        } )
    } )
  }

  function makeSetDataTask ( options ) {
    return function setDataTask ( taskComplete ) {
      buckets.child( options.site ).child( options.key ).child( 'dev/data' )
        .set( options.data, function ( error ) {
          var responseData = { site: options.site };
          if ( error ) responseData.error( error );
          taskComplete( null, responseData )
        } )
    }
  }

  function isWYSIWYG ( value ) {
    if ( typeof value === 'string' && value.length > 0 ) {
      return value.indexOf( '<p>' ) === 0 || value.indexOf( '<figure' ) === 0;
    }
    return false;
  }

  function updateHTML ( html ) {
    var $ = cheerio.load( html );
    $( 'a' ).each( function ( index, element ) {
      var href = $( this ).attr( 'href' )
      $( this ).attr( 'href', updateUrl( href ) )
    } )
    $( 'img' ).each( function ( index, element ) {
      var src = $( this ).attr( 'src' )
      $( this ).attr( 'href', updateUrl( src ) )
    } )
    return $.html()
  }

  function isUrlKey ( path ) {
    return path.slice( -1 )[ 0 ] === 'url';
  }

  function updateUrl ( url ) {
    var uploadsDirectory = '/webhook-uploads/';
    if ( ! ( typeof url === 'string' && url.length > 0 ) ) return url;
    if ( url.indexOf( uploadsDirectory ) === 0 ) {
      return [ '//', uploadsBucket, url ].join( '' );
    }
    else if ( typeof migratingFrom === 'string' && migratingFrom.length > 0 && url.indexOf( uploadsDirectory ) > 0 ) {
      // in case the uploads bucket changes
      // https://domain.com/other-location/webhook-uploads/path-to-file.ext
      var urlParts = url.split( uploadsDirectory )
      // See if the domain we are migrating from is in the domain side of
      // URL parts. If not, we shouldn't consider this a migrated file.
      if ( urlParts[0].indexOf( migratingFrom ) === -1 ) return url;

      return [ '//', uploadsBucket, uploadsDirectory, urlParts[ 1 ] ].join( '' )
    }
    else {
      return url;
    }
  }
}
